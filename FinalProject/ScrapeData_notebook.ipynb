{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using textual analysis of financial documents to gain insight on a company's market performance\n",
    "Individual student final project, Physics 250 Econophysics, Winter 2017\n",
    "\n",
    "Dustin Burns, PhD Candidate, UC Davis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "The goal of this study is to measure the correlation between negative tone in a company's SEC filing reports and the short-term market return of the company's stock, reproducing the results found by [Loughran and McDonald](https://www.uts.edu.au/sites/default/files/ADG_Cons2015_Loughran%20McDonald%20JE%202011.pdf).\n",
    "\n",
    "Tone is measured in the documents using a \"bag-of-words\" algorithm, counting the number of negative words in the document which appear in a custom finance dictionary, defined [here](http://www3.nd.edu/~mcdonald/Word_Lists_files/LoughranMcDonald_MasterDictionary_2014.\n",
    "xlsx).\n",
    "\n",
    "The list of data files and metadata used to crosscheck this analysis can be found [here](http://www3.nd.edu/~mcdonald/Data/LoughranMcDonald_10X_2014.xlsx), and the directory structure of the SEC Edgar database is described [here](https://www.sec.gov/edgar/searchedgar/accessing-edgar-data.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from yahoo_finance import Share\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import wget\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = pd.read_excel('data/LoughranMcDonald_MasterDictionary_2014.xlsx', sheetname=0)\n",
    "fin_neg = dictionary.Word[dictionary.Negative > 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'UNCOVER', u'DISAPPEARANCES', u'DISMISSES', u'PANIC', u'DEADWEIGHT', u'EXPLOITATIONS', u'CLOSED', u'UNREIMBURSED', u'INCIDENCE', u'UNCOLLECTED', u'RECALLS', u'TERMINATION', u'ABDICATION', u'UNJUST', u'CONFINING', u'SUMMONSES', u'ERRATIC', u'INORDINATELY', u'INSUBORDINATION', u'EXACERBATE', u'INACTIVATIONS', u'FELONIES', u'MISAPPROPRIATING', u'UNDERMINE', u'CATASTROPHIC', u'CONFESSED', u'OVERLOAD', u'OVERAGES', u'DEADLOCK', u'CONCILIATING']\n"
     ]
    }
   ],
   "source": [
    "# What are some example negative words?\n",
    "print random.sample(fin_neg, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load first 10 rows for demo\n",
    "data = pd.read_excel('data/LoughranMcDonald_10X_2014_test_1.xlsx', sheetname=0, skip_header=1)\n",
    "\n",
    "# Load N randomly sampled rows for full study\n",
    "#N = 5000\n",
    "#data = pd.read_excel('data/LoughranMcDonald_10X_2014.xlsx', sheetname=0, skip_header=1)\n",
    "#data = data.loc[random.sample(list(data.index), N)]\n",
    "\n",
    "# Drop unused cols, copy cols to crosscheck\n",
    "data = data.drop(['N_Positive', 'N_Uncertainty', 'N_Litigious', 'N_WeakModal', 'N_StrongModal', 'N_Constraining', 'N_Negation', 'GrossFileSize', 'NetFileSize', 'ASCIIEncodedChars', 'HTMLChars', 'XBRLChars', 'TableChars'], axis=1)\n",
    "data['N_Words_Paper'] = data.N_Words\n",
    "data['N_Unique_Words_Paper'] = data.N_Unique_Words\n",
    "data['N_Negative_Paper'] = data.N_Negative\n",
    "data = data.drop(['N_Negative'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert filing date to datetime\n",
    "data.FILING_DATE  = data.FILING_DATE.astype('string')\n",
    "for time in data.FILING_DATE:\n",
    "  t = time[0:4] + '-' + time[4:6] + '-' + time[6:8]\n",
    "  data.ix[data.FILING_DATE == time, 'FILING_DATE'] = t\n",
    "data.FILING_DATE  = pd.to_datetime(data.FILING_DATE)\n",
    "\n",
    "# End date is 3 days after filing date\n",
    "data['End_Date'] = data.FILING_DATE + timedelta(days=3)\n",
    "\n",
    "# Skip weekends\n",
    "#print data.FILING_DATE\n",
    "#print data.End_Date\n",
    "#\n",
    "#delta = timedelta(days=1)\n",
    "#for ind, date in enumerate(data.End_Date):\n",
    "#  i=0\n",
    "#  while i < 4:\n",
    "#    if date.weekday() not in [5, 6]:\n",
    "#      print data.End_Date[ind].weekday()\n",
    "#      data.End_Date[ind] = data.End_Date[ind] + delta\n",
    "#      i += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape and clean data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions:\n",
    "\n",
    "# Return counts of words in dictionary\n",
    "def count_words(toks, dictionary):\n",
    "  cnts = [0]*len(dictionary)\n",
    "  for tok in toks:\n",
    "    if tok.upper() in dictionary: cnts[dictionary.tolist().index(tok.upper())] += 1\n",
    "  return cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [$, $, $, $, $, $, $, $, $, $, $, $, $, $, $, ...\n",
      "1    [$, $, $, $, $, $, $, $, $, $, $, $, $, $, $, ...\n",
      "2    [$, $, $, $, $, $, $, $, $, $, $, $, $, $, $, ...\n",
      "3    [$, $, $, $, $, $, $, $, $, $, $, $, $, $, $, ...\n",
      "4    [$, $, $, $, $, $, $, $, $, $, $, $, $, $, $, ...\n",
      "5    [$, $, $, $, $, $, $, $, $, $, $, $, $, $, $, ...\n",
      "6    [#, #, #, $, $, $, $, $, $, $, $, $, $, $, $, ...\n",
      "7    [$, $, $, $, $, $, $, $, $, $, $, $, $, $, $, ...\n",
      "8    [$, $, $, $, $, $, $, $, $, $, $, $, $, $, $, ...\n",
      "9    [!, !, !, !, $, $, $, $, $, $, $, $, $, $, $, ...\n",
      "Name: Toks_Dirty, dtype: object\n",
      "0    [ACT, ALFERONR, AMENDMENT, AND, ANNUAL, ASSETS...\n",
      "1    [ALFERONR, AMENDMENT, ANALYSIS, AND, ASSETS, A...\n",
      "2    [ACT, ACTIVITIES, ADJUSTED, ANALYSIS, AND, APP...\n",
      "3    [ACT, ANALYSIS, AND, ASSETS, Accounts, Act, Ad...\n",
      "4    [ACCOUNTANTS, ACT, AID, AND, APB, ASSETS, AUDI...\n",
      "5    [ACCOUNTANTS, ACCOUNTING, ACT, ACTIVITIES, AL,...\n",
      "6    [ACT, AGGREGATED, AGREEMENTS, AND, ANNUAL, AT,...\n",
      "7    [ABOVE, ACCOUNTANTS, ACCOUNTING, ACT, AGE, AGE...\n",
      "8    [ACT, ADF, AEL, ANALYSIS, AND, ASSETS, Account...\n",
      "9    [ACCOUNTS, ACT, ACTIVITIES, AND, ASSETS, AT, A...\n",
      "Name: Toks_Unique, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Analysis arrays\n",
    "toks_raw_dirty = []\n",
    "toks_raw_clean = []\n",
    "toks_total = []\n",
    "toks_unique = []\n",
    "toks_freq = []\n",
    "avg_freq = []\n",
    "neg_counts = []\n",
    "neg_total = []\n",
    "\n",
    "# Loop through metadata rows\n",
    "for name in data.FILE_NAME:\n",
    "  \n",
    "  # Build file name\n",
    "  fname = 'https://www.sec.gov/Archives/edgar/data/' + name.split('data_')[1].replace('_','/')\n",
    "  data.ix[data.FILE_NAME == name, 'FILE_NAME'] = fname\n",
    "  \n",
    "  # Download file\n",
    "  try: \n",
    "    f = 'data/10X/' + fname.split('/')[-1]\n",
    "  #  f = wget.download(fname, './data/10X')\n",
    "  except: continue\n",
    "  \n",
    "  # Raw toks before cleaning\n",
    "  text = open(f).read()\n",
    "  toks_dirty = sorted(np.array(nltk.word_tokenize(text)))\n",
    "  toks_raw_dirty.append(toks_dirty)\n",
    "\n",
    "  # Remove non-text HTML blocks\n",
    "  # TO DO: compile regex before loop to speed up\n",
    "  os.rename(f, f + '.orig')\n",
    "  with open(f + '.orig', 'rb') as fin, open(f, 'wb') as fout:\n",
    "    text = fin.read()\n",
    "    text = re.sub(r'<IMS-HEADER>.*?</IMS-HEADER>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<SEC-HEADER>.*?</SEC-HEADER>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<GRAPHIC>.*?</GRAPHIC>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<ZIP>.*?</ZIP>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<EXCEL>.*?</EXCEL>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<PDF>.*?</PDF>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<XBRL>.*?</XBRL>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<.*?>', '', text, flags=re.DOTALL)\n",
    "    fout.write(text)\n",
    "  os.remove(f + '.orig')\n",
    "\n",
    "  text = open(f).read()\n",
    "  try: toks = np.array(nltk.word_tokenize(text))\n",
    "  except: continue\n",
    "  \n",
    "  # Remove words that contain numbers or special characters\n",
    "  special_inds = [not bool(re.search('[\\d\\/\\*\\'\\-,=;:@<>\\.\\_]', x)) for x in toks]  \n",
    "  try: toks = toks[np.array(special_inds)]\n",
    "  except: pass\n",
    "\n",
    "  # Remove single char words\n",
    "  word_inds = [len(x)>1 for x in toks]\n",
    "  toks = toks[np.array(word_inds)]\n",
    "  \n",
    "  # Remove negated words here for positive words\n",
    "  # ['no', 'not', 'none', 'neither', 'never', 'nobody'] occurs four or fewer words before\n",
    "   \n",
    "  # Alphabetize\n",
    "  toks = sorted(toks) \n",
    "\n",
    "  # Fill total word counts\n",
    "  toks_raw_clean.append(toks)\n",
    "  toks_total.append(len(toks))\n",
    "  #data.ix[data.FILE_NAME == fname, 'N_Words'] = len(toks) \n",
    "    \n",
    "  # Fill unique word counts\n",
    "  toks_dict = OrderedDict(sorted(Counter(toks).items()))\n",
    "  toks_freq.append(toks_dict.values())\n",
    "  avg_freq.append(np.nanmean(toks_dict.values()))\n",
    "  toks_unique.append(toks_dict.keys())\n",
    "  #data.ix[data.FILE_NAME == fname, 'N_Unique_Words'] = len(toks_dict.keys()) \n",
    "  \n",
    "  # Fill negative word counts\n",
    "  neg_in_dict = count_words(toks, fin_neg)\n",
    "  neg_counts.append(neg_in_dict)\n",
    "  neg_total.append(sum(neg_in_dict))\n",
    "\n",
    "data['Toks_Dirty'] = toks_raw_dirty\n",
    "data['Toks_Raw'] = toks_raw_clean\n",
    "data['Toks_Total'] = toks_total\n",
    "data['Toks_Freq'] = toks_freq\n",
    "data['Avg_Freq'] = avg_freq\n",
    "data['Toks_Unique'] = toks_unique\n",
    "data['Neg_Counts'] = neg_counts\n",
    "data['Neg_Total'] = neg_total\n",
    "\n",
    "print data.Toks_Dirty\n",
    "print data.Toks_Unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words crosscheck\n",
      "[13159  3423  2231  1851  2428  5127  3303 31494  2938  2858]\n",
      "[11222  2705  1698  1206  1783  4163  2347 25479  2310  2252]\n",
      "\n",
      "Negative words crosscheck\n",
      "[153  24  21  26  11  98  21 267  50  10]\n",
      "[142  21  21   7  11  90  21 240  43   8]\n"
     ]
    }
   ],
   "source": [
    "print \"Total words crosscheck\"\n",
    "print data.Toks_Total.values\n",
    "print data.N_Words_Paper.values\n",
    "print\n",
    "print \"Negative words crosscheck\"\n",
    "print data.Neg_Total.values\n",
    "print data.N_Negative_Paper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate proportional yield, weighting negative word count by document length\n",
    "data['Prop_Yield'] = data.Neg_Total / data.Toks_Total\n",
    "data['Prop_Yield_Paper'] = data.N_Negative_Paper / data.N_Words_Paper\n",
    "\n",
    "# Calculate tf.idf yield, weighting word count by term frequency and inverse document frequency\n",
    "# number of documents containing 1+ occurance of ith word\n",
    "df = [0] * len(dictionary)\n",
    "for cnts in data.Neg_Counts:\n",
    "  for i, cnt in enumerate(cnts):\n",
    "    if cnt > 0: df[i] += 1\n",
    "# Number of documents\n",
    "N = len(data.CIK.values)\n",
    "dict_len = len(fin_neg)\n",
    "# Calculate sum of tf.idf weights\n",
    "w = [0] * N\n",
    "for j in range(0, N):\n",
    "  weights = [0] * dict_len\n",
    "  for i in range(0, dict_len):\n",
    "    tf = data.Neg_Counts.values[j][i]\n",
    "    if tf > 0:\n",
    "      weights[i] = (1 + np.log(tf)) * np.log(N / df[i]) / (1 + np.log(data.Avg_Freq[j]))\n",
    "  w[j] = sum(weights)\n",
    "data['Neg_Yield'] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape historical stock prices\n",
    "Historical stock data is acquired from the Yahoo Finance database with the python plugin [yahoo-finance](https://github.com/lukaszbanasiak/yahoo-finance).\n",
    "\n",
    "The key for converting SEC CIK number to ticker symbol is obtained [here](http://rankandfiled.com/#/data/tickers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to convert CIK number to ticker symbol\n",
    "key = pd.read_csv('data/cik_ticker.csv', sep='|')  \n",
    "def cik_to_ticker(cik):\n",
    "  key.CIK = key.CIK.astype('str')\n",
    "  try: \n",
    "    ind = key.CIK[key.CIK == cik].index.tolist()[0]\n",
    "    ticker = key.Ticker[ind]\n",
    "  except IndexError: \n",
    "    ticker = 'NaN'\n",
    "  return ticker\n",
    "\n",
    "# Convert CIK to ticker symbol, delete row if missing from key\n",
    "nan_ind = []\n",
    "tickers = []\n",
    "for ind, cik in enumerate(data.CIK): \n",
    "  ticker = cik_to_ticker(str(cik))\n",
    "  if ticker == 'NaN': nan_ind.append(ind)\n",
    "  else: tickers.append(ticker)\n",
    "data = data.drop(data.index[nan_ind])\n",
    "data['Ticker'] = tickers\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get historical returns from yahoo_finance API \n",
    "filing_return = []\n",
    "market_return = []\n",
    "for ind, ticker in enumerate(data.Ticker):\n",
    "  share = Share(ticker)\n",
    "  market = Share('^GSPC')\n",
    "  try: \n",
    "    returns = share.get_historical(str(data.FILING_DATE[ind])[0:10], str(data.End_Date[ind])[0:10])\n",
    "    market = market.get_historical(str(data.FILING_DATE[ind])[0:10], str(data.End_Date[ind])[0:10])\n",
    "  except: \n",
    "    filing_return.append(float('NaN'))\n",
    "    market_return.append(float('NaN'))\n",
    "    continue\n",
    "  returns  = pd.DataFrame(returns)\n",
    "  returns.Adj_Close = returns.Adj_Close.astype(float)\n",
    "  filing_return.append( ((returns.Adj_Close.tail(1).values - returns.Adj_Close.head(1).values) / returns.Adj_Close.head(1).values)[0] )\n",
    "  market = pd.DataFrame(market)\n",
    "  market.Adj_Close = market.Adj_Close.astype(float)\n",
    "  market_return.append( ((market.Adj_Close.tail(1).values - market.Adj_Close.head(1).values) / market.Adj_Close.head(1).values)[0])\n",
    "data['Filing_Return'] = filing_return\n",
    "data['Market_Return'] = market_return\n",
    "data['Excess_Return'] = data.Filing_Return - data.Market_Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean any rows with missing data\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "  \n",
    "# Write final dataframe to file\n",
    "data.to_csv(path_or_buf='data/cleaned_10X_test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "#data1 = pd.read_csv('data/cleaned_10X_1.csv')\n",
    " \n",
    "(a, b, r, tt, stderr) = stats.linregress(data.Neg_Yield, data.Excess_Return)\n",
    "print 'Slope:           ' + str(a)\n",
    "print 'Standard error:  ' + str(stderr)\n",
    "print 'R-squared:       ' + str(r**2)\n",
    "print 'p-value:         ' + str(tt)\n",
    "\n",
    "plt.figure()\n",
    "#ax = sns.regplot(x=\"Prop_Yield_Paper\", y=\"Excess_Return\", data=data, x_bins=5, ci=68)\n",
    "ax = sns.regplot(x=\"N_Negative_Paper\", y=\"Excess_Return\", data=data, x_bins=5, ci=68, x_estimator=np.median)\n",
    "#plt.xlabel('Weighted negative word count', horizontalalignment='right', x=1.0, fontsize=16)\n",
    "plt.xlabel('Unweighted negative word count', horizontalalignment='right', x=1.0, fontsize=16)\n",
    "#plt.xlabel('Proportional weighted negative word count', horizontalalignment='right', x=1.0, fontsize=16)\n",
    "#plt.ylabel('Filing period excess return (%)', horizontalalignment='right', y=1.0, fontsize=16)\n",
    "plt.ylabel('Mean filing period excess return (%)', horizontalalignment='right', y=1.0, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product ideation\n",
    "\n",
    "My results are consistent with those found by [Loughran and McDonald](https://www.uts.edu.au/sites/default/files/ADG_Cons2015_Loughran%20McDonald%20JE%202011.pdf): negative tone in SEC filings is weakly correlated with the filing period excess return, but not enough to completely inform a trading strategy.\n",
    "\n",
    "However, a strong negative sentiment of a company's filings CAN be used to red flag potential weaknesses in a portfolio. Below, I outline a product using the techniques shown above, to be used by investors to inform their portfolio management decision making:\n",
    "\n",
    "* For each company's stock in portfolio, periodically check for SEC filing\n",
    "* If recent filing exists, calculate negative word count\n",
    "* If negative word count above threshold, red flag company for further investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
